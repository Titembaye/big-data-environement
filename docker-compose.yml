version: "2.4"

services:
  namenode:
    build: ./namenode
    container_name: namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./data/:/hadoop-data/input
      - ./map_reduce/:/hadoop-data/map_reduce
    environment:
      - CLUSTER_NAME=test
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - hadoop_network

  resourcemanager:
    build: ./resourcemanager
    container_name: resourcemanager
    restart: on-failure
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    ports:
      - "8089:8088"
    networks:
      - hadoop_network

  historyserver:
    build: ./historyserver
    container_name: historyserver
    depends_on:
      - namenode
      - datanode1
      - datanode2
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    ports:
      - "8188:8188"
    networks:
      - hadoop_network

  nodemanager1:
    build: ./nodemanager
    container_name: nodemanager1
    depends_on:
      - namenode
      - datanode1
      - datanode2
    ports:
      - "8042:8042"
    networks:
      - hadoop_network

  datanode1:
    build: ./datanode
    container_name: datanode1
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    networks:
      - hadoop_network

  datanode2:
    build: ./datanode
    container_name: datanode2
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data
    networks:
      - hadoop_network

  datanode3:
    build: ./datanode
    container_name: datanode3
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode3:/hadoop/dfs/data
    networks:
      - hadoop_network

  spark:
    build: ./spark
    container_name: spark
    ports:
      - "7077:7077"
      - "8080:8080"
      - "4040:4040"
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
      - resourcemanager
      - nodemanager1
      - historyserver
    networks:
      - hadoop_network
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_SUBMIT_OPTIONS=--conf spark.driver.maxResultSize=2g --conf spark.executor.memoryOverhead=512m

  spark-worker1:
    image: bitnami/spark:latest
    container_name: spark-worker1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    networks:
      - hadoop_network

  spark-worker2:
    image: bitnami/spark:latest
    container_name: spark-worker2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    networks:
      - hadoop_network

  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - hadoop_network

  pig:
    build: ./pig
    container_name: pig
    depends_on:
      - namenode
      - resourcemanager
      - spark
    volumes:
      - ./pig_data:/data
    networks:
      - hadoop_network

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  hadoop_datanode3:
  hadoop_historyserver:
  pig_data:
  mongo_data:
    driver: local

networks:
  hadoop_network:
    name: hadoop_network
    external: true
